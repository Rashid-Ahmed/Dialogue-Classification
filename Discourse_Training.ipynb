{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"F1jeeNUzlhXL"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Rashi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["#!pip install fasttext\n","\n","import pandas as pd\n","import torch\n","import os\n","from utils.get_embeddings import get_model, vector_embeddings\n","from utils.data_processing import get_one_hot, get_split_tensors, get_splitted_data\n","from utils.train_model import get_model_weights, initialize_model, train_model, load_existing_model"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FQN-A_vMlhXO"},"outputs":[],"source":["EMBEDDING_SIZE = 100\n","SENTENCE_SIZE = 79\n","DATA_DIR = 'data'\n","EMBEDDING_MODEL_NAME = 'cc.de.100.bin'\n","EPI_DATA_DIR = os.path.join(os.getcwd(), DATA_DIR, 'Augmented_Data_EPI.csv')\n","SOC_DATA_DIR = os.path.join(os.getcwd(), DATA_DIR, 'Augmented_Data_SOC.csv')\n","EPI_DATA_LEN = 16857\n","SOC_DATA_LEN = 17103\n","CHECKPOINT_DIR = os.path.join(os.getcwd(), 'checkpoints')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9649,"status":"ok","timestamp":1666945246354,"user":{"displayName":"rashid nizamani","userId":"08693151884265058089"},"user_tz":-120},"id":"MUqtNhyalhXP","outputId":"538bd934-a6a4-4177-9f95-95e1d1e30e78"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#device = 'cpu'\n","embedding_model = get_model(os.path.join(DATA_DIR, EMBEDDING_MODEL_NAME))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"u8xsDaeTlhXQ"},"outputs":[],"source":["\n","NUM_LAYERS = 2\n","HIDDEN_SIZE = 400\n","STEP_SIZE =  0.0003\n","\n","EPOCHS = 2\n","BATCH_SIZE = 512\n","OUTPUT_SIZE_EPI = 2\n","OUTPUT_SIZE_SOC = 6\n","MODEL_TYPES = ['self', 'parents', 'teacher', 're', 'cause', 'none', 'soc']\n","\n","\n","data_epi = pd.read_csv(EPI_DATA_DIR)\n","data_epi = get_one_hot(data_epi['epi'])\n","data_soc = None\n","\n","def start_processes(TYPE_MODEL, data, DATA_FILE, DATA_LEN,load_model = False):\n","    data, OUTPUT_SIZE = get_splitted_data(TYPE_MODEL, data)\n","    model_weights = None\n","    if TYPE_MODEL!='soc':\n","        model_weights = get_model_weights(data)\n","    model, criterion, optimizer = initialize_model(device,OUTPUT_SIZE ,HIDDEN_SIZE , NUM_LAYERS, EMBEDDING_SIZE, STEP_SIZE, model_weights = model_weights)\n","    if load_model == True:\n","        model = load_existing_model(os.path.join('checkpoints', 'model_'+TYPE_MODEL+'.ckpt'), device, OUTPUT_SIZE_EPI, HIDDEN_SIZE , NUM_LAYERS, EMBEDDING_SIZE, STEP_SIZE)\n","\n","    train_model(model, optimizer, criterion, TYPE_MODEL, BATCH_SIZE, EPOCHS, SENTENCE_SIZE, EMBEDDING_SIZE, embedding_model, DATA_FILE, CHECKPOINT_DIR, DATA_LEN, device)\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772720,"status":"ok","timestamp":1666946058952,"user":{"displayName":"rashid nizamani","userId":"08693151884265058089"},"user_tz":-120},"id":"O2VGVq5AlhXS","outputId":"f68a9774-201b-49f2-84af-248b655a8319"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"ename":"TypeError","evalue":"can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#start_processes('parents', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#start_processes('none', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#start_processes('self', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#start_processes('teacher', data_epi, EPI_DATA_DIR, EPI_DATA_LEN,  load_model = False)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#start_processes('re', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m#start_processes('cause', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m start_processes(\u001b[39m'\u001b[39m\u001b[39msoc\u001b[39m\u001b[39m'\u001b[39m, data_soc, SOC_DATA_DIR, SOC_DATA_LEN, load_model \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n","Cell \u001b[1;32mIn [4], line 25\u001b[0m, in \u001b[0;36mstart_processes\u001b[1;34m(TYPE_MODEL, data, DATA_FILE, DATA_LEN, load_model)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m load_model \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     model \u001b[39m=\u001b[39m load_existing_model(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mTYPE_MODEL\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.ckpt\u001b[39m\u001b[39m'\u001b[39m), device, OUTPUT_SIZE_EPI, HIDDEN_SIZE , NUM_LAYERS, EMBEDDING_SIZE, STEP_SIZE)\n\u001b[1;32m---> 25\u001b[0m train_model(model, optimizer, criterion, TYPE_MODEL, BATCH_SIZE, EPOCHS, SENTENCE_SIZE, EMBEDDING_SIZE, embedding_model, DATA_FILE, CHECKPOINT_DIR, DATA_LEN, device)\n","File \u001b[1;32md:\\EDUTECH\\chatbot\\utils\\train_model.py:35\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, criterion, TYPE_MODEL, BATCH_SIZE, EPOCHS, SENTENCE_SIZE, EMBEDDING_SIZE, EMBEDDING_MODEL, DATA_FILE, CHECKPOINT_DIR, DATA_LEN, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m vectors, targets \u001b[39m=\u001b[39m dataset[i]\n\u001b[0;32m     34\u001b[0m embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(vectors)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 35\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(targets)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m train_batches:\n\u001b[0;32m     38\u001b[0m   \u001b[39m#Training the model\u001b[39;00m\n\u001b[0;32m     39\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n","\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."]}],"source":["#start_processes('parents', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\n","#start_processes('none', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\n","#start_processes('self', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\n","#start_processes('teacher', data_epi, EPI_DATA_DIR, EPI_DATA_LEN,  load_model = False)\n","#start_processes('re', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\n","#start_processes('cause', data_epi, EPI_DATA_DIR, EPI_DATA_LEN, load_model = False)\n","start_processes('soc', data_soc, SOC_DATA_DIR, SOC_DATA_LEN, load_model = False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"7ddf133b4eda059986521323f3292d266b4e97de12a5e96c4c9cf48956d966ac"}}},"nbformat":4,"nbformat_minor":0}
